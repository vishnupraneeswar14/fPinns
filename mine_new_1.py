# -*- coding: utf-8 -*-
"""Copy of Final_Fracbeam_Fourier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w3nbMGJsLDbRvTsuM0VzOx0BVTj7VNOw

Modules imported
"""

import torch
import torch.nn as nn
import random
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from scipy.special import gamma
import scipy
from tqdm import tqdm
import time
import math
import pandas as pd

# Add ConFIG imports
from conflictfree.momentum_operator import PseudoMomentumOperator
from conflictfree.grad_operator import ConFIG_update
from conflictfree.utils import OrderedSliceSelector, get_gradient_vector, apply_gradient_vector

torch.set_default_dtype(torch.float64)
 
if torch.cuda.is_available():
    device = torch.device('cuda')
    print('CUDA')
else:
    device = torch.device('cpu')
    print('CPU')

def euler_beam(E,rho,l,b,h):
    I=b*(h**3)/12
    A=b*h
    kb=(E*I/(l**3))*torch.tensor([[12,6*l,-12,6*l],[6*l,4*l**2,-6*l,2*l**2],[-12,-6*l,12,-6*l],[6*l,2*l**2,-6*l,4*l**2]])
    mb=((rho*A*l)/420)*torch.tensor([[156,22*l,54,-13*l],[22*l,4*l**2,13*l,-3*l**2],[54,13*l,156,-22*l],[-13*l,-3*l**2,-22*l,4*l**2]])
    return kb,mb

def globassemble(kb,mb,nel,enod,dof,bc):
  CM=torch.zeros((nel,3),dtype=int)
  for i in range(1,nel+1):
    CM[i-1]=torch.tensor([i,i,i+1],dtype=int)
  nnod=nel+1
  Kg = torch.zeros(dof * nnod, dof * nnod)
  Mg = torch.zeros(dof * nnod, dof * nnod)
  for ii in range(nel):
    node=CM[ii,1:]
    for jj in range(enod):
      for kk in range(enod):
         # Global stiffness matrix assembly
                Kg[dof * (node[jj] - 1): dof * (node[jj]),
           dof * (node[kk] - 1): dof * (node[kk])] += kb[dof * jj: dof * (jj + 1),
                                                       dof * kk: dof * (kk + 1)]
        # Global mass matrix assembly
                Mg[dof * (node[jj] - 1): dof * (node[jj]),
           dof * (node[kk] - 1): dof * (node[kk])] += mb[dof * jj: dof * (jj + 1),
                                                       dof * kk: dof * (kk + 1)]
  mask = torch.ones(Kg.shape[0], dtype=torch.bool)
  mask[bc] = False
  Kg2 = Kg[mask][:, mask]
  Mg2 = Mg[mask][:, mask]
  return Kg2,Mg2

def globassemble_womask(kb,mb,nel,enod,dof):
  CM=torch.zeros((nel,3),dtype=int)
  for i in range(1,nel+1):
    CM[i-1]=torch.tensor([i,i,i+1],dtype=int)
  nnod=nel+1
  Kg = torch.zeros(dof * nnod, dof * nnod)
  Mg = torch.zeros(dof * nnod, dof * nnod)
  for ii in range(nel):
    node=CM[ii,1:]
    for jj in range(enod):
      for kk in range(enod):
         # Global stiffness matrix assembly
                Kg[dof * (node[jj] - 1): dof * (node[jj]),
           dof * (node[kk] - 1): dof * (node[kk])] += kb[dof * jj: dof * (jj + 1),
                                                       dof * kk: dof * (kk + 1)]
        # Global mass matrix assembly
                Mg[dof * (node[jj] - 1): dof * (node[jj]),
           dof * (node[kk] - 1): dof * (node[kk])] += mb[dof * jj: dof * (jj + 1),
                                                       dof * kk: dof * (kk + 1)]
  return Kg,Mg

def fraccaputo_V4(yt,h,a,tau,k):
    device=yt.device
    st=time.time()
    fracorder=torch.zeros(yt.shape[0],1)
    mem=torch.tensor([0,0])
    mem[0]=torch.max(torch.tensor(0),k-torch.ceil(tau/h))
    mem[1]=k
    k_st=mem[1]-mem[0]

    j=torch.arange(k_st,device=device)
    j = torch.flip(j, dims=[0])
    w=torch.zeros_like(j,dtype=yt.dtype, device=device)
    w[0]=(j[0] + 2) ** (1 - a) - 3 * (j[0]+1) ** (1 - a) + 2*j[0] ** (1 - a)
    w[1:-1]=(j[1:-1] +1) ** (1 - a) - 2 * (j[1:-1] ) ** (1 - a) + (j[1:-1]-1) ** (1 - a)
    w[-1]=1
    fracorder=(h ** (-a) / torch.exp(torch.lgamma(2 - a)))*torch.sum(yt[:,-k_st:]*w,dim=1,keepdim=True)
    ed=time.time()
    ittim=ed-st
    return fracorder

def fracviscobeam(M,K,C,F,dof,nnod,dt,x0,v0,T,a,tau):
   device= torch.device("cuda" if torch.cuda.is_available() else "cpu")
   print({device})
   Nt = int(round(T/dt))
   u = torch.zeros(dof * nnod -2, Nt)
   frac=torch.zeros_like(v0)
   u[:,0] = x0.squeeze()
   u[:,1] = u[:,0] + dt*v0.squeeze() + (dt**2)*torch.linalg.solve(M,(F[:,0].unsqueeze(1)-torch.matmul(K,u[:,0].unsqueeze(1)) - torch.matmul(C,v0))).squeeze()
   for n in tqdm (range (1,Nt-1),
               desc="Loading",
               ascii=False, ncols=75):
      frac=fraccaputo_V4( u[:,:n], dt, a, tau, n)
      u[:,n+1]=2*u[:,n] - u[:,n-1] + (dt**2)*torch.linalg.solve(M,(F[:,n].unsqueeze(1)-torch.matmul(K,u[:,n].unsqueeze(1)) - torch.matmul(C,frac))).squeeze()
      if torch.isnan(u).any():
            print(f"\nInstability detected at iteration {n + 1}: NaN value encountered.")
            return u
   return u

def autograd_matrix(u,t):
    grad_u=torch.zeros_like(u)
    for i in range(u.shape[1]):
        grad_u[:,i]=torch.autograd.grad(u[:,i].view(-1,1),t,torch.ones_like(u[:,i].view(-1,1)),create_graph=True)[0].squeeze()
    return grad_u

def autograd_matrix_v2(u, t):
  grad_u = torch.zeros_like(u)
  dt = t[1] - t[0]
  grad_u[1:-1, :] = (u[2:, :] - u[:-2, :]) / (2 * dt)
  grad_u[0, :] = (u[1, :] - u[0, :]) / dt
  grad_u[-1, :] = (u[-1, :] - u[-2, :]) / dt
  return grad_u

def autograd_matrix_second_v2(u, t):
  grad_u = torch.zeros_like(u)
  dt = t[1] - t[0]
  grad_u[1:-1,:] = (u[2:,:] - 2*u[1:-1,:]+u[:-2,:]) / (dt**2)
  grad_u[0, :] = (-u[3, :] +4 * u[2, :] -5*u[1,:]+ 2*u[0, :]) / (dt**2)
  grad_u[-1, :] = (2*u[-1, :] - 5* u[-2, :] +4*u[-3,:]-u[-4, :]) / (dt**2)
  return grad_u

def fraccaputo_V6(yt, h, a, tau):
      device = yt.device
      ytlen = yt.shape[0]
      w=torch.eye(ytlen)
      k=torch.arange(ytlen)
      cols = torch.arange(ytlen).unsqueeze(0).expand(ytlen, -1)
      k_st=k - torch.max(torch.tensor(0),k-torch.ceil(tau/h))
      mask= (cols >= (k - k_st + 1).unsqueeze(1)) & (cols <= (k -1).unsqueeze(1))
      j=torch.zeros(ytlen, ytlen)
      j = (k.unsqueeze(1) - cols -1)[mask]
      kk = k.long()[1:]
      kk_st = k_st.long()[1:]
      w[kk.long(),(kk-kk_st).long()]=(kk_st + 1) ** (1 - a) - 3* (kk_st) ** (1 - a) + 2*(kk_st - 1) ** (1 - a)
      w[mask]=((j+2) ** (1 - a ) - 2 * (j+1) ** (1 - a ) + (j) ** (1 - a ))
      w[0,0]=0
      w=(h ** (-a) / torch.exp(torch.lgamma(2 - a))) *  w
      fracorder=torch.matmul(w,yt.float())
      return fracorder

def fraccaputo_V7(yt,h,a,tau):
    m=yt.shape[0]
    Time_frame_size = int(torch.ceil(tau/h))
    power_vector = torch.arange(Time_frame_size + 3) # 0 1 2 3 ... K + 2
    power_vector = torch.pow(power_vector,1 - a)
    Weight_vector = (power_vector[2:] - 2*power_vector[1:-1] + power_vector[:-2])[:Time_frame_size]
    Weight_vector[Time_frame_size - 1] = Weight_vector[Time_frame_size - 1] + power_vector[Time_frame_size - 1] - power_vector[Time_frame_size]
    Weight_vector = Weight_vector.flip([0])
    Weight_vector = torch.cat([Weight_vector,torch.tensor([1])])
    if (Time_frame_size != m):
        Weight_vector = torch.cat([Weight_vector,torch.zeros(m - Time_frame_size - 1)])
    row = torch.arange(m)
    idx = (row.unsqueeze(1).flip([0]) + row + Time_frame_size + 1) % m
    mat = Weight_vector[idx]
    k_temp = row.long()[:Time_frame_size - 1] + 1
    mat[k_temp,0] += -power_vector[k_temp] + power_vector[k_temp - 1]
    mat.diagonal().copy_(torch.ones(m))
    mat[0,0] = 0
    mat = torch.tril(mat)*(h ** (-a) / torch.exp(torch.lgamma(2 - a)))
    fracorder=torch.matmul(mat,yt)
    return fracorder

class Sine(nn.Module):
  def __init__(self, input_size, output_size,dominant_freq):
        super().__init__()
        self.linear = nn.Linear(input_size, output_size)
  def forward(self, x,dominant_freq):
        return torch.sin(2*torch.pi*dominant_freq*self.linear(x))

class Net(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, layers_num, dominant_freq, initial_a2_value):
        super(Net, self).__init__()
        activation = nn.Tanh # You can experiment with Sine or Tanh
        # activation2 = Sine
        # Fourier Feature layer
        # mapping_size determines the number of Fourier features (sine and cosine pairs)
        # We can relate the scale to the dominant frequency
        # fourier_mapping_size = 8 # You can adjust this
        # fourier_scale = 2 * torch.pi * dominant_freq # Scale related to dominant frequency
        self.sine_layer = Sine(input_size, hidden_size,dominant_freq)

        # Input layer (receives output from FourierFeatureLayer)
        # input_after_fourier = 2# because we have sine and cosine pairs
        self.layers = nn.ModuleList()
        self.dominant_freq = dominant_freq
        # Hidden layers
        for _ in range(layers_num):
            self.layers.extend([nn.Linear(hidden_size, hidden_size), activation()])

        # Output layer
        self.layers.append(nn.Linear(hidden_size, output_size))

        self.a2 = torch.nn.Parameter(torch.tensor(initial_a2_value, dtype=torch.float64, device=device), requires_grad=True)

        # torch.nn.init.xavier_normal_(self.layers.weight[:], gain = 5/3)
        # torch.nn.init.constant_(self.layers.bias[:], 0)
        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight, gain=5/3)
                nn.init.constant_(layer.bias, 0)

    def forward(self, x):
        x = self.sine_layer(x,self.dominant_freq)
        for layer in self.layers:
            x = layer(x)
        return x


# class MConFIG:
#     def __init__(self, model_params, num_terms, lr, beta1, beta2, eps):
#         """
#         M-ConFIG optimizer implementation following Algorithm 1 from ConFIG paper
        
#         Args:
#             model_params: Model parameters (PINN + fractional order a2)
#             num_terms: Number of loss terms (default: 5)
#             lr: Learning rate (default: 5e-4)
#             beta1: First momentum decay rate (default: 0.9)
#             beta2: Second momentum decay rate (default: 0.999)
#             eps: Epsilon for numerical stability (default: 1e-8)
#         """
#         self.model_params = list(model_params)
#         self.num_terms = num_terms
#         self.lr = lr
#         self.beta1 = beta1
#         self.beta2 = beta2
#         self.eps = eps
        
#         # Algorithm 1 variables
#         self.t = 0  # Global time step
#         self.param_dim = sum(p.numel() for p in self.model_params)
        
#         # Initialize momentum buffers for each loss term
#         self.mg = [torch.zeros(self.param_dim) for _ in range(num_terms)]  # First momentum for each loss
#         self.tg = [0] * num_terms  # Time counters for each gradient
        
#         # Pseudo first momentum and second momentum
#         self.m = torch.zeros(self.param_dim)  # Pseudo first momentum
#         self.v = torch.zeros(self.param_dim)  # Second momentum
        
#         # SGD optimizer (as recommended by paper)
#         self.optimizer = optim.SGD(self.model_params, lr=lr)
    
#     def zero_grad(self):
#         """Zero gradients for all parameters"""
#         for param in self.model_params:
#             if param.grad is not None:
#                 param.grad.zero_()
    
#     def step(self, compute_losses_fn):
#         """
#         Execute one M-ConFIG optimization step following Algorithm 1
        
#         Args:
#             compute_losses_fn: Function that returns list of all 5 losses
#         """

#         # with torch.no_grad():
#         self.t += 1
        
#         # Algorithm 1, line 3: i = t%m + 1 (alternating loss selection)
#         i = (self.t - 1) % self.num_terms
        
#         # Compute all losses for monitoring
#         all_losses = compute_losses_fn()
#         selected_loss = all_losses[i]
        
#         # Algorithm 1, line 4: tgi ← tgi + 1
#         self.tg[i] += 1
        
#         # Zero gradients and compute gradient for selected loss
#         self.zero_grad()
#         selected_loss.backward(retain_graph=True)
        
#         # Algorithm 1, line 5: Update first momentum of gi
#         gi = get_gradient_vector(self.model_params)
#         self.mg[i] = self.beta1 * self.mg[i] + (1 - self.beta1) * gi
        
#         # Algorithm 1, lines 6-7: Bias corrections for first momentum terms
#         mg_corrected = []
#         for j in range(self.num_terms):
#             if self.tg[j] > 0:
#                 mg_corrected.append(self.mg[j] / (1 - self.beta1**self.tg[j]))
#             else:
#                 mg_corrected.append(torch.zeros_like(self.mg[j]))
        
#         # Algorithm 1, line 8: ConFIG update of momentums
#         if all(self.tg[j] > 0 for j in range(self.num_terms)):
#             # All losses have been computed at least once
#             # Stack momentum gradients for ConFIG_update
#             mg_stack = torch.stack(mg_corrected)  # Shape: (num_terms, param_dim)
#             mg = ConFIG_update(mg_stack)  # ConFIG_update expects stacked gradients
#         else:
#             # Not all losses computed yet, use current gradient
#             mg = gi
        
#         # Algorithm 1, line 9: Obtain estimated gradient
#         gc = (mg * (1 - self.beta1) - self.beta1 * self.m) / (1 - self.beta1)
        
#         # Algorithm 1, line 10: Update pseudo first momentum
#         self.m = self.beta1 * self.m + (1 - self.beta1) * gc
        
#         # Algorithm 1, line 11: Update second momentum
#         self.v = self.beta2 * self.v + (1 - self.beta2) * (gc ** 2)
        
#         # Algorithm 1, line 12: Bias correction for second momentum
#         v_corrected = self.v / (1 - self.beta2**self.t)
        
#         # Algorithm 1, line 13: Update weights
#         update = -self.lr * self.m / (torch.sqrt(v_corrected) + self.eps)
#         apply_gradient_vector(self.model_params, update)
            
#         return all_losses


# class MConFIGWrapper:
#     """Wrapper class for easier integration with existing code"""
#     def __init__(self, model_params, num_terms, lr, beta1, beta2, eps):
#         self.mconfig = MConFIG(model_params, num_terms, lr, beta1, beta2, eps)
    
#     def step(self, compute_losses_fn):
#         return self.mconfig.step(compute_losses_fn)


"""Data Generation for Fractional Euler beam"""

F0,E,L,rho=100,2e11,1,7.8e3
b,h=L/40,L/40
dof,nel=2,2
nnod=nel+1
l=L/nel
bc=torch.tensor([0,dof*nel])
Kbe,Mbe=euler_beam(E,rho,l,b,h)
Kg,Mg=globassemble(Kbe,Mbe,nel,2,dof,bc)

eigenvalues, eigenvectors = torch.linalg.eig(torch.linalg.solve(Mg, Kg))
freq = torch.sqrt(eigenvalues)/(2*torch.pi)

freq = freq.real
zeta = 0.04
a1 = 2*zeta*freq[-1]*freq[-2]/(freq[-1]+freq[-2])
b1 = 2*zeta/(freq[-1]+freq[-2])

Cg=a1*Mg + b1*Kg
x0=torch.zeros(dof*nnod -2,1)
v0=torch.zeros(dof*nnod -2,1)
T=1.
a_copy=0.6
a=torch.tensor([a_copy])
tau=torch.tensor([T])
dt=0.00005
tvec=torch.arange(0,T,dt)
Fg=torch.zeros(dof*nnod -2,len(tvec))
Fg[nnod-2,:]=F0*torch.sin(2*math.pi*freq[-1]*tvec)
Fg[nnod-2,0]=F0*100
u=fracviscobeam(Mg,Kg,Cg,Fg,dof,nnod,dt,x0,v0,T,a,tau)
# plt.plot(tvec,u[nnod-2,:])
data = {'tvec': tvec, 'u_fdm': u[nnod-2,:]}
df = pd.DataFrame(data)
df.to_csv('tensor_data3.csv', index=False)
#plt.show()

"""Frsactional Alpha Prediction using PINNs"""

torch.manual_seed(123)
#Read observation data from excel
excel_file_path = f'tensor_data3.csv'
df = pd.read_csv(excel_file_path)
t1 = df['tvec'].values
u1 = df['u_fdm'].values
t_obs = torch.from_numpy(t1)
u_obs = torch.from_numpy(u1)
t_obs = t_obs.unsqueeze(1)
u_obs = u_obs.unsqueeze(1)

# PINN Initialisation and Observations data
a2=torch.nn.Parameter(torch.tensor(0.9),requires_grad=True)
a1s=[]
tau=torch.tensor([T])
#tau1s=[]

t_phy =torch.linspace(0,T,1000,requires_grad=True).view(-1,1)  # Training time steps
t_test=torch.linspace(0,T,5000).view(-1,1) # Testing time steps

# Complete System Matrices
Kg2,Mg2=globassemble_womask(Kbe,Mbe,nel,2,dof)
Cg2=a1*Mg2 + b1*Kg2
midnode = (dof*(nnod-1))//2


# PINN instance for alpha prediction

pinn=Net(1,200,dof*nnod,4,freq[-1], 0.9)
# M-ConFIG setup
# mconfig = MConFIGWrapper(list(pinn.parameters()) + [a2], num_terms=5, lr=5e-4, beta1=0.9, beta2=0.999, eps = 1e-8)
# optimiser=torch.optim.SGD(list(pinn.parameters()),lr=1e-4)  # Optimiser parameters defn

# Boundary conditions
bc_val = torch.tensor([0])  #start and end nodes are fixed
# Initial conditions
u_val = torch.tensor([0])
v_val = torch.tensor([0])



# Monitoring lists
l1s, l2s, l3s, l4s, l5s = [], [], [], [], []
losses = []
iters = []
a1s = []

epochs = 200000

# Assume 'device' is defined globally (e.g., device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
# Assume derivative helper functions are defined globally:
# autograd_matrix_v2(u_series_mine, t_vec)
# autograd_matrix_second_v2(u_series_mine, t_vec)
# fraccaputo_V7(yt_mine, h_scalar, a_scalar_tensor, tau_scalar_tensor)

class loss_module(nn.Module):
    def __init__(self, Mg2_full, Kg2_full, Cg2_full, F0_val, freq_tensor, tau_pinn_val, 
                 midnode_idx, bc_indices_cpu, u_ic_val, v_ic_val, bc_target_val, h_for_frac_deriv_val,
                 dof_val, nnod_val, global_device):
        super().__init__()
        self.global_device = global_device # Store the global device

        # Store constants and matrices needed for loss calculations, ensuring they are on the correct device
        self.Mg2_full = Mg2_full.to(self.global_device)
        self.Kg2_full = Kg2_full.to(self.global_device)
        self.Cg2_full = Cg2_full.to(self.global_device)
        
        # Scalar values
        self.F0 = F0_val 
        self.midnode = midnode_idx
        self.dof = dof_val
        self.nnod = nnod_val
        
        # Tensors to be moved to device
        self.freq_loss_component = freq_tensor[-1].to(self.global_device) # Assuming freq_tensor is like the 'freq' variable and we need the last component
        self.tau_pinn = tau_pinn_val.to(self.global_device)
        self.bc_cpu = bc_indices_cpu # Keep on CPU for numpy-style indexing if that's how it's used, or move to device if used with tensor indexing
        self.u_val_pinn = u_ic_val.to(self.global_device)
        self.v_val_pinn = v_ic_val.to(self.global_device)
        self.bc_val_pinn = bc_target_val.to(self.global_device)
        self.h_for_frac_deriv = h_for_frac_deriv_val.to(self.global_device)


    # Loss L1: Initial Displacement
    # Inputs for this loss: t_phy (tensor of time points for physical domain)
    def L1(self, network, inputs):
        t_phy = inputs # Assuming inputs is t_phy
        t_phy = t_phy.to(self.global_device)
        u = network(t_phy) # u shape: (len(t_phy), dof*nnod)
        # IC is typically at t=0. u[0,:] gives all DOF values at the first time point in t_phy.
        return torch.mean((u[0,:] - self.u_val_pinn)**2)

    # Loss L2: Initial Velocity
    # Inputs for this loss: t_phy
    def L2(self, network, inputs):
        t_phy = inputs
        t_phy = t_phy.to(self.global_device)
        u = network(t_phy)
        # Pass u.T (shape: dof*nnod, len(t_phy)) to autograd_matrix_v2
        dudt_mine = autograd_matrix_v2(u, t_phy) # dudt_mine shape: (dof*nnod, len(t_phy))
        # dudt_mine[:,0] gives velocities of all DOFs at the first time point.
        return torch.mean((dudt_mine[0,:] - self.v_val_pinn)**2)

    # Loss L3: Boundary Conditions
    # Inputs for this loss: t_phy
    def L3(self, network, inputs):
        t_phy = inputs
        t_phy = t_phy.to(self.global_device)
        u = network(t_phy) # u shape: (len(t_phy), dof*nnod)
        # bc_cpu contains indices of DOFs that are constrained.
        # u[:, self.bc_cpu.long()] selects these specific DOF columns for all time points.
        u_boundary_selected_dofs = u[:, self.bc_cpu.long().cpu()] # .cpu() if bc_cpu is numpy array or for advanced indexing with CPU tensor
        return torch.mean((u_boundary_selected_dofs - self.bc_val_pinn)**2)

    # Loss L4: PDE Residual (Physics Loss)
    # Inputs for this loss: t_phy
    def L4(self, network, inputs):
        t_phy = inputs
        t_phy = t_phy.to(self.global_device)
        u = network(t_phy) # u shape: (len(t_phy), dof*nnod)
        
        # Forcing term, assuming F0 acts at midnode
        # Make sure t_phy.squeeze() has the right shape for element-wise multiplication with sin output
        F_phy2_loss = torch.zeros(self.dof*self.nnod, len(t_phy), device=self.global_device, dtype=torch.float64)
        F_phy2_loss[self.midnode] = self.F0 * torch.sin(2 * torch.pi * self.freq_loss_component * t_phy).squeeze()
        
        # Derivatives (expect u.T which is (dof*nnod, len(t_phy)))
        dudt_mine = autograd_matrix_v2(u, t_phy)
        # network.a2 is the fractional order from the PINN model
        dudt_frac_mine = fraccaputo_V7(u, self.h_for_frac_deriv, network.a2, self.tau_pinn)
        du2dt_mine = autograd_matrix_second_v2(u, t_phy)
        
        # PDE Residual calculation
        # Shapes: Mg2_full (N,N), du2dt_mine (N,T), Kg2_full (N,N), u.T (N,T), etc.
        # F_phy2_loss (N,T) where N = dof*nnod
        R_phy = (torch.matmul(self.Mg2_full, du2dt_mine.T) + 
                 torch.matmul(self.Kg2_full, u.T) +
                 torch.matmul(self.Cg2_full, dudt_frac_mine.T) - 
                 F_phy2_loss) 
        return torch.mean(R_phy**2)

    # Loss L5: Observational Data
    # Inputs for this loss: (t_obs, u_obs_target)
    def L5(self, network, t_obs, u_obs_target):
        # t_obs, u_obs_target = inputs # Expecting a tuple

        t_obs = t_obs.to(self.global_device)
        u_obs_target = u_obs_target.to(self.global_device)

        u_obs_pred = network(t_obs) # u_obs_pred shape: (len(t_obs), dof*nnod)
        # Select displacement at midnode for comparison with u_obs_target
        u_nod_pred = u_obs_pred[:, self.midnode].view(-1,1)
        return torch.mean((u_nod_pred - u_obs_target)**2)


loss_instance = loss_module(
    Mg2_full=Mg2,                 # From globassemble_womask
    Kg2_full=Kg2,                 # From globassemble_womask 
    Cg2_full=Cg2,                 # Calculated from full Mg2, Kg2 and a1, b1
    F0_val=F0,                    # Global F0
    freq_tensor=freq,             # Global freq tensor (loss_module will take freq[-1])
    tau_pinn_val=tau,             # Global tau (used as tau_pinn)
    midnode_idx=midnode,          # Global midnode index
    bc_indices_cpu=bc,            # Global bc tensor (for CPU-based indexing)
    u_ic_val=u_val,               # PINN initial condition for u
    v_ic_val=v_val,               # PINN initial condition for v
    bc_target_val=bc_val,         # PINN boundary condition target value
    h_for_frac_deriv_val=torch.tensor([0.001], device=device, dtype=torch.float64), # Example h
    dof_val=dof,                  # Global dof
    nnod_val=nnod,                # Global nnod
    global_device=device          # Global device object
)

loss_fns = [loss_instance.L1, loss_instance.L2, loss_instance.L3, loss_instance.L4, loss_instance.L5]
    
# current_param_dim = sum(p.numel() for p in pinn.parameters()) # Calculate after pinn is fully defined
# momentum_operator=PseudoMomentumOperator(num_vectors=len(loss_fns))
# loss_selector=OrderedSliceSelector()
optimizer=torch.optim.SGD(pinn.parameters(), lr=1e-5)

# for i in range(1,epochs+1):
#     if i%len(loss_fns) == 0:
#         iters.append(i)
#     st=time.time()
#     loss_index,input_loss_fn=loss_selector.select(1,loss_fns)
#     optimizer.zero_grad()

#     loss_index = loss_index[0] + 1
#     # print(loss_index)

#     if loss_index != len(loss_fns):
#         loss=input_loss_fn(pinn, t_phy)
#     else:
#         loss=input_loss_fn(pinn, t_obs, u_obs)

#     eval(f'l{loss_index}s').append(loss.item())
#     loss_index -= 1
#     loss.backward()    

#     a1s.append(pinn.a2.item())

#     for p_val in pinn.parameters():
#         if p_val.grad is None:
#             p_val.grad = torch.zeros_like(p_val.data, device=p_val.device, dtype=p_val.dtype)

#     current_grad_vector = get_gradient_vector(pinn)
    
#     # CRITICAL: Check if gradient vector size matches param_dim_override
#     if current_grad_vector.numel() != current_param_dim:
#         print(f"ERROR in loop: get_gradient_vector size {current_grad_vector.numel()} != expected {current_param_dim}")

#     momentum_operator.update_gradient(pinn,[loss_index],grads=[current_grad_vector])
#     optimizer.step()

#     ed = time.time()
#     iteration_time = ed - st

#     if i % 2500 == 0:
#         print(f"Training step {i}, Time taken: {iteration_time:.4f} s, Alpha: {pinn.a2.item():.4f}")
        
#         u = pinn(t_test).detach()
#         plt.figure(figsize=(20, 20))
#         plt.subplot(2,1,1)
#         plt.plot(t_test[:,0], u[:,midnode])
#         plt.scatter(t_obs, u_obs, c='red', s=1)
#         plt.legend(["PINN","Observations"])
#         plt.title(f"Training step {i}")
#         plt.subplot(2, 1, 2)
#         plt.plot(a1s)
#         plt.hlines(a, 0, len(a1s))
#         plt.legend(["PINN","Exact"])
#         plt.title(f"Alpha Prediction: {pinn.a2.item():.4f}")
#         plt.savefig(f'iter_{i}_{a}_n.png')
#         plt.close()
        
#         plt.figure(figsize=(20, 20))
#         for j in range(dof*nnod):
#             plt.subplot(2,5,j+1)
#             plt.plot(t_test[:,0], u[:,j])
#             plt.title(f'Node Number = {j}')
#         plt.savefig(f'iter_{i}_{a}_nod_disp.png')
#         plt.close()
        
#         plt.figure(figsize=(20, 20))
#         plt.semilogy(iters, l1s)
#         plt.semilogy(iters, l2s)
#         plt.semilogy(iters, l3s)
#         plt.semilogy(iters, l4s)
#         plt.semilogy(iters, l5s)
#         plt.legend(['L1','L2','L3','L4','L5'])
#         plt.savefig(f'iter_loss_{i}.png')
#         plt.close()

for i in range(epochs):
    grads=[]
    iters.append(i)
    st = time.time()
    for loss_index in range(len(loss_fns)):
        optimizer.zero_grad()
        for p_val in pinn.parameters():
            if p_val.grad is None:
                p_val.grad = torch.zeros_like(p_val.data, device=p_val.device, dtype=p_val.dtype)
        
        if loss_index != len(loss_fns)-1:
            loss_i=loss_fns[loss_index](pinn, t_phy)
        else:
            loss_i=loss_fns[loss_index](pinn, t_obs, u_obs)
        # loss_i=loss_fn(pinn,input_fn())
        loss_i.backward()
        loss_index = loss_index + 1
        eval(f'l{loss_index}s').append(loss_i.item())
        loss_index = loss_index - 1
        grads.append(get_gradient_vector(pinn))
    apply_gradient_vector(pinn,ConFIG_update(grads))
    optimizer.step()

    ed = time.time()
    iteration_time = ed - st

    if i % 99 == 0:
        print(f"Training step {i}, Time taken: {iteration_time:.4f} s, Alpha: {pinn.a2.item():.4f}")
        
        u = pinn(t_test).detach()
        plt.figure(figsize=(20, 20))
        plt.subplot(2,1,1)
        plt.plot(t_test[:,0], u[:,midnode])
        plt.scatter(t_obs, u_obs, c='red', s=1)
        plt.legend(["PINN","Observations"])
        plt.title(f"Training step {i}")
        plt.subplot(2, 1, 2)
        plt.plot(a1s)
        plt.hlines(a, 0, len(a1s))
        plt.legend(["PINN","Exact"])
        plt.title(f"Alpha Prediction: {pinn.a2.item():.4f}")
        plt.savefig(f'iter_{i}_{a_copy}_n.png')
        plt.close()
        
        plt.figure(figsize=(20, 20))
        for j in range(dof*nnod):
            plt.subplot(2,5,j+1)
            plt.plot(t_test[:,0], u[:,j])
            plt.title(f'Node Number = {j}')
        plt.savefig(f'iter_{i}_{a_copy}_nod_disp.png')
        plt.close()
        
        plt.figure(figsize=(20, 20))
        plt.semilogy(iters, l1s)
        plt.semilogy(iters, l2s)
        plt.semilogy(iters, l3s)
        plt.semilogy(iters, l4s)
        plt.semilogy(iters, l5s)
        plt.legend(['L1','L2','L3','L4','L5'])
        plt.savefig(f'iter_loss_{i}_{a_copy}.png')
        plt.close()

